# -*- coding: utf-8 -*-
"""CDU.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1gmLm4nNX-6rWh0h8h4TY30vLnVuP3aDx

#CDU - AnimeCom
---

O dataset pode ser encontrado em: [Dataset-animes-com-reviews](https://www.kaggle.com/marlesson/myanimelist-dataset-animes-profiles-reviews).

A pasta com os arquivos necessários e/ou gerados pode ser encontrada em: [Pasta](https://drive.google.com/drive/folders/1nRKQuBNi5TnnOq-UyFLxn3YxwHmsMuvH?usp=sharing)

Relatórios: 
* [animes.](https://datastudio.google.com/reporting/a298d91e-d999-4e51-be4c-9c8377dc2858)

* [reviews.](https://datastudio.google.com/reporting/63a6c5ec-bbf4-4f37-bc60-74b722075594)

* [profile.](https://datastudio.google.com/reporting/6b2f4515-7fa0-4fbd-ac7c-eba39c83a7a8)
"""

#Acesso ao drive
from google.colab import drive
drive.mount('/content/drive')

"""# Análise do dataset"""

#importação de bibliotecas usadas nessa sessão
import pandas as pd
import numpy as np
import datetime
import matplotlib.dates as mdates

"""## Exploração dos dados

Nessa seção faremos uma análise inicial do dataset.

O dataset é composto por 03 arquivos csv:

- reviews.csv

  Este arquivo contém 192112 linhas e 7 colunas, nele tem dados sobre os reviews que os usuários deram para os animes como scores e texto do usuário.

- profile.csv

 Este arquivo contém 81727 linhas × 5 colunas, nele tem dados sobre os perfils dos usuários como genero, aniversário e animes favoritos.  


- animes.csv

  Este arquivo contém 19311 linhas × 12 colunas, nele tem dados sobre os animes como score, ranking, sinopse e episodios. 
"""

#Carregamento do dataset
base_reviews = pd.read_csv('/content/drive/MyDrive/PISI3/dataset/reviews.csv')
base_animes = pd.read_csv('/content/drive/MyDrive/PISI3/dataset/animes.csv')
base_profile = pd.read_csv('/content/drive/MyDrive/PISI3/dataset/profiles.csv')

"""### Visualização inicial do dataset

#### reviews

Ao observar o arquivo **reviews.csv** podemos identificar que a base de dados tem variáveis numéricas e categóricas e a base possui alguns atributos que não serão interessantes para esse momento da análise, como por exemplo o link do review ou o texto do review.
"""

base_reviews

"""Classificação dos atributos do arquivo reviews.csv:

uid - Variável Númerica discreta.

profile	- Variável categórica nominal.

anime_uid	- Variável Númerica discreta.

text	- Variável categórica nominal.

score	- Variável Númerica contínua.

scores - 	Variável categórica ordinal.

link - Variável categórica nominal.

#### profiles

Ao observar o arquivo **profiles.csv** podemos identificar que a base de dados tem soemente variáveis categóricas, podemos observar que a base contém alguns registros com atributos nulos, além disso, a base possui alguns atributos que não serão interessantes para esse momento da análise, como por exemplo o link do perfil.
"""

base_profile

"""Classificação dos atributos do arquivo profile.csv:
profile	- Variável categórica nominal.

gender - Variável categórica nominal.

birthday	- Variável categórica nominal.

favorites_anime	- Variável categórica nominal.

link - Variável categórica nominal.

#### animes

Ao observar o arquivo **animes.csv** podemos identificar que a base de dados tem variáveis numéricas e categóricas e a base possui alguns atributos que não serão interessantes para esse momento da análise, como por exemplo o link do perfil ou o link da imagem do anime.
"""

base_animes

"""Classificação dos atributos do arquivo animes.csv:

uid	- Variável númerica contínua.

title	- Variável categórica nominal.

synopsis - Variável categórica nominal.

genre	- Variável categórica nominal.

aired	- Variável categórica nominal.

episodes - Variável númerica discreta.

members	- Variável númerica discreta.

popularity -  Variável númerica discreta.

ranked -  Variável númerica discreta.

score	- Variável númerica discreta.

img_url	- Variável categórica nominal.

link - Variável categórica nominal.

### Descrição inicial dos dados
Nessa seção iremos obter uma descrição de como o dataset está inicialmente.

#### reviews
"""

base_reviews.describe(include='all')

"""#### profiles"""

base_profile.describe(include='all')

"""#### animes"""

base_animes.describe(include='all')

"""## Pré-processamento para a construção do relatório

### Removendo Duplicidades

Nessa seção serão removidos registros que estão duplicados no dataset.
"""

base_reviews.drop_duplicates(inplace=True)
base_profile.drop_duplicates(inplace=True)
base_animes.drop_duplicates(subset=['title'], inplace=True)

"""###Verificando valores nulos.

Nessa seção vamos lidar com registros que contém atributos com valores nulos.

Ao analisar a base de reviews não foram encontrados registros com valores nulos.
"""

base_reviews.isna().sum()

"""Ao analisar a base de profile e animes foi possível identificar que os atributos gender e birthday que representam o gênero e a data de nascimento do usuário respectivamente da base profile e os atributos episodes, ranked e score que representam a quantidade de episodios, o lugar no ranking e o score dos animes respectivamente da base de animes continham valores nulos, para não correr o risco de adulterar os dados não será efetuado nenhuma correção nesses atributos já que esses valores não serão considerados no momento das análises e geração dos gráficos e os registros contém dados potencialmente relevantes nos outros atributos."""

base_profile.isna().sum()

base_animes.isna().sum()

"""### Verificando valores inconsistentes
Nessa seção será verificado se o padrão dos atributos estão corretos, se não, então será corrigido.

#### reviews
"""

base_reviews.loc[base_reviews['score'] > 10]

"""Ao observar a base de dados de reviews na descrição inicial feita anteriormente foi possível notar que um registro está fora do padrão de score que é um valor entre 0 e 10, para não comprometer os parâmetros da análise esse único registro foi removido."""

base_reviews.drop(base_reviews.loc[base_reviews['score'] > 10].index, inplace=True)

"""Como não foi gerada nenhuma exceção com a execução do código, pode-se concluir que não foi encontrado erro de variáveis inconsistente na base de reviews, isso significa que os valores na base seguem o tipo esperado."""

for r in base_reviews.values: #Para cada registro na base
  
  #r[0] = uid, se o uid for diferente de um inteiro então está fora do padrão 
  if type(r[0]) != int:
    raise Exception(f" uid fora do padrão - {r}") 

  #r[1] = profile, se o profile for diferente de uma string então está fora do padrão 
  if type(r[1]) != str:
    raise Exception(f" profile fora do padrão - {r}") 

  #r[2] = anime_id, se o anime_id for diferente de um inteiro então está fora do padrão 
  if type(r[2]) != int :
    raise Exception(f" anime_id fora do padrão - {r}")

  #r[3] - text, se o text for diferente de uma string então está fora do padrão
  if type(r[3]) != str:
    raise Exception(f" text fora do padrão - {r}") 

  #r[4] = score, se o score for diferente de um inteiro então está fora do padrão 
  if type(r[4]) != int :
    raise Exception(f" score fora do padrão - {r}")

  #r[5] = scores, se estiver diferente de um dicionário então está fora do padrão 
  r[5] = eval(r[5])
  if type(r[5]) != dict:
    raise Exception(f" scores fora do padrão - {r}")
  keys = ['Overall', 'Story', 'Animation', 'Sound', 'Character', 'Enjoyment']
  for k in r[5].keys():
    if k not in keys or keys.count(k) > 1: #Verificando a quantidade e se as chaves estão corretas
      raise Exception(f" scores fora do padrão - {r}")
  for v in r[5].values():
    if not v.isdigit(): #Verificando se o tipo dos valores estão correto
     raise Exception(f" scores fora do padrão - {r[5]}")

  #r[6] - link, se o link for diferente de uma string então está fora do padrão
  if type(r[6]) != str:
    raise Exception(f" link fora do padrão - {r}")

"""A coluna scores contém a pontuação de aspectos do anime como história, animação e personagens, optamos por desmembrar essa coluna em 5 novas colunas: Story, Animation, Sound, Character, Enjoyment. """

base_reviews.insert(5,'story', 0)
base_reviews.insert(6,'animation', 0)
base_reviews.insert(7,'sound', 0)
base_reviews.insert(8,'character', 0)
base_reviews.insert(9,'enjoyment', 0)

for i in base_reviews.index:
  scores = eval(base_reviews.scores[i])
  base_reviews.story[i] = int(scores['Story'])
  base_reviews.animation[i] = int(scores['Animation'])
  base_reviews.sound[i] = int(scores['Sound'])
  base_reviews.character[i] = int(scores['Character'])
  base_reviews.enjoyment[i] = int(scores['Enjoyment'])

base_reviews.drop('scores', 1, inplace=True)

"""#### profiles

Como não foi gerada nenhuma exceção com a execução do código, pode-se concluir que não foi encontrado erro de variáveis inconsistente na base de profiles, isso significa que os valores na base seguem o tipo esperado.
"""

for p in base_profile.values: #Para cada registro na base

  #p[0] = profile, se profile for diferente de uma string então está fora do padrão
  if type(p[0]) != str:
    raise Exception(f" profile fora do padrão - {p}")

  #p[1] = gender, se gender for diferente de uma string então está fora do padrão
  if str(p[1]) !='nan'and type(p[1]) != str:
    raise Exception(f" gender fora do padrão - {p}")

  #p[2] = birthday, se birthday for diferente de uma string então está fora do padrão
  if str(p[2]) !='nan'and type(p[2]) != str:
    raise Exception(f" birthday fora do padrão - {p}")

  #p[3] = favorites_anime, se favorites_anime for diferente de uma Lista então está fora do padrão
  favorites_anime = eval(p[3])
  if type(favorites_anime) != list:
    raise Exception(f" favorites_anime fora do padrão - {favorites_anime}")    
    
  #id = id dos animes, se id for diferente de uma string com dígitos então está fora do padrão
  for id in favorites_anime:
    id = int(id)
    if len(favorites_anime) > 0 and type(id) != int:
      raise Exception(f" favorites_anime fora do padrão - {favorites_anime}")
    
  #p[4] = link, se link for diferente de uma string então está fora do padrão
  if type(p[4]) != str:
    raise Exception(f" link fora do padrão - {p}")

"""#### animes

Como não foi gerada nenhuma exceção com a execução do código, pode-se concluir que não foi encontrado erro de variáveis inconsistente na base de animes, isso significa que os valores na base seguem o tipo esperado, porém o atributo aired que representa a duração do anime tem dados diferentes (não atómicos) em relação aos animes que já foram encerrados e animes que ainda estão em lançamento, por exemplo, se o anime já foi encerrado o atributo conterá a data de inicio e de encerramento do anime, se o anime ainda estava em lançamento no periodo da extração do dataset o atributo conterá somente a data de inicio do anime.
"""

for a in base_animes.values: #Para cada registro na base

  #a[0] = uid, se diferente de um inteiro então está fora do padrão.
  if type(a[0]) != int:
    raise Exception(f" uid fora do padrão - {a}")

  #a[1] = title, se diferente de uma string então está fora do padrão.
  if type(a[1]) != str:
      raise Exception(f" title fora do padrão - {a}")
  
  #a[2] = synopsis, se diferente de uma string então está fora do padrão.
  if str(a[2]) !='nan'and type(a[2]) != str:
      raise Exception(f" synopsis fora do padrão - {a}")

  #a[3] = genre, se genre for diferente de uma Lista então está fora do padrão
  genre = eval(a[3])
  if type(genre) != list:
    raise Exception(f" genre fora do padrão - {a}")
  #d = genero do anime, se g for diferente de uma string então está fora do padrão
  for g in genre:
    if len(genre) > 0 and (type(g) != str):
      raise Exception(f" genre fora do padrão - {a}")

  #a[4] = aired, se aired for diferente de uma string então está fora do padrão
  if type(a[4]) != str:
    raise Exception(f" aired fora do padrão - {a}")

  #a[5] = episodes, se episodes for diferente de um número então está fora do padrão
  if str(a[5]) !='nan'and type(a[5]) != float:
    raise Exception(f" episodes fora do padrão - {a}")

  #a[6] = members, se members for diferente de um número inteiro então está fora do padrão
  if type(a[6]) != int:
    raise Exception(f" members fora do padrão - {a[6]}")

  #a[7] = popularity, se popularity for diferente de um número inteiro então está fora do padrão
  if type(a[7]) != int:
    raise Exception(f" popularity fora do padrão - {a[7]}")

  #a[8] = ranked, se ranked for diferente de um número então está fora do padrão
  if str(a[8]) !='nan'and type(a[8]) != float:
    raise Exception(f" ranked fora do padrão - {a}")

  #a[9] = score, se score for diferente de um número então está fora do padrão
  if str(a[9]) !='nan'and type(a[9]) != float:
    raise Exception(f" score fora do padrão - {a}")
  
  #a[10] = img_url, se diferente de uma string então está fora do padrão.
  if str(a[10]) !='nan'and type(a[10]) != str:
      raise Exception(f" img_url fora do padrão - {a}")
    
  #a[11] = link, se diferente de uma string então está fora do padrão.
  if type(a[11]) != str:
      raise Exception(f" link fora do padrão - {a}")

"""Pelo fato do atributo aired ter mais dados em animes que já foram finalizados, o atributo foi dividido em 2 novos atributos (start, finish)."""

base_animes.insert(3,'start', 0)
base_animes.insert(4,'finish', 0)

for i in base_animes.index:
  durac = base_animes.aired[i]
  if ' to' in durac and '?' not in durac:
    inicio, fim = durac.split(' to')
    base_animes.start[i] = inicio.strip()
    base_animes.finish[i] = fim.strip()
  elif '?' in durac:
    base_animes.start[i] = durac.split(' to')[0]
  elif 'Not available' == durac:
    pass    
  else:
    base_animes.start[i] = durac

base_animes.drop('aired', 1, inplace=True)

"""### Visualização e exportação dos dados pré-processados"""

base_reviews.to_csv(r'/content/drive/MyDrive/PISI3/dataset/dataset-analise/reviews-limpo.csv', index=False)
base_reviews

base_profile.to_csv('/content/drive/MyDrive/PISI3/dataset/dataset-analise/profiles-limpo.csv', index=False)
base_profile

base_animes.to_csv('/content/drive/MyDrive/PISI3/dataset/dataset-analise/animes-limpo.csv', index=False)
base_animes

"""##Análise gráfica"""

#importação de bibliotecas
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from matplotlib.colors import ListedColormap
import plotly.express as px
import matplotlib.patches as mpatches
from scipy.stats import kde
from datetime import date
import ast
import spacy
import csv
from spacy.lang.en.stop_words import STOP_WORDS
from wordcloud import WordCloud

base_reviews = pd.read_csv('/content/drive/MyDrive/PISI3/dataset/dataset-analise/reviews-limpo.csv')
base_animes = pd.read_csv('/content/drive/MyDrive/PISI3/dataset/dataset-analise/animes-limpo.csv')
base_profile = pd.read_csv('/content/drive/MyDrive/PISI3/dataset/dataset-analise/profiles-limpo.csv')

"""### Top 10 gêneros por quantidade de animes:"""

genero = {}
for i in base_animes.values:  
  generos = eval(i[5])
  for g in generos:
    if g not in genero:
      genero[g] = 1
    else:
      genero[g] = genero[g] + 1

genero = [[k,genero[k]] for k in genero.keys()]
df = pd.DataFrame(genero, columns=['genero', 'quantidade'])

df.sort_values(by=['quantidade'], inplace=True, ascending=False)
df.head(10)

"""### Popularidade do anime por quantidade de episódios"""

x = base_animes.episodes.values
y = abs(base_animes.popularity.values - len(base_animes.popularity.values))

mask1 = y < 15000
mask2 = y >= 15000

plt.scatter(x[mask1], y[mask1], color = 'grey')
plt.scatter(x[mask2], y[mask2], color = 'green')

plt.xticks(rotation=90)
plt.xlim([0,400])
plt.title('Grafico de popularidade por quantidade de episódios')
plt.xlabel("episódios")
plt.ylabel("popularidade")
green_patch = mpatches.Patch(color='green', label='Animes mais populares')
plt.legend(handles=[green_patch], loc= "upper right")
plt.subplots_adjust(left=0.0, right=2.0, bottom=0.0, top=1.0)
plt.show()

"""### Estimativa de densidade kernel do score dos animes"""

data = base_animes.score

density = kde.gaussian_kde([float(i) for i in data if float(i) > 0 ]) 
x = np.linspace(0,10) 
y=density(x)

plt.plot(x,y) 
plt.title("Densidade do score dos animes") 
plt.show()

"""### Quantidade de animes por gênero:"""

plt.bar(df.genero.values, df.quantidade.values)
plt.xticks(rotation=90)
plt.xlabel('Gênero')
plt.ylabel('Quantidade de animes no dataset')
plt.title('Grafico de gênero de anime por quantidade')
plt.subplots_adjust(left=0.0, right=2.0, bottom=0.0, top=1.0)
plt.show()

"""### Relação entre os animes com maior score e a quantidade de episódios."""

df1 = base_animes.loc[base_animes['score'] != np.nan]
df1 = df1.loc[df1['episodes'] != np.nan]
df1 = df1.loc[df1['ranked'] != np.nan]

g = px.scatter_matrix(df1, dimensions=['episodes', 'score',], color='ranked', title='Gráfico que relaciona o score com a quantidade de episódios e ranked de um anime.')
g.show()

"""### Score médio dos gêneros de animes."""

#genero, anime_uid
genero = {}
for i in base_animes.values:
  #score_anime
  score_anime = i[10]
  if str(score_anime) == 'nan' : continue
  #Conta os generos
  generos = eval(i[5])
  for g in generos:
    if g not in genero:
      genero[g] = [score_anime]
    else:
      genero[g].append(score_anime)

import statistics

for key in genero.keys():
  genero[key] = statistics.mean(genero[key])

genero = [[k,genero[k]] for k in genero.keys()]
df2 = pd.DataFrame(genero, columns=['genero', 'score'])
df2.sort_values(by=['score'], inplace=True, ascending=False)
plt.plot(df2.genero.values, df2.score.values)
plt.xticks(rotation=90)
plt.xlabel('Gênero')
plt.ylabel('Score médio')
plt.title('Gráfico de score médio dos gêneros.')
plt.subplots_adjust(left=0.0, right=2.5, bottom=0.0, top=1.0)
plt.show()

"""###Quantidade animes lançados anualmente agrupados por gênero."""

genero = {}
for i in base_animes.values:  
  generos = eval(i[5]) #generos
  if i[3].strip() != '0' : 
    ano = i[3].split(',')[-1].strip()

    for g in generos:
      if g not in genero:
        genero[g] = [ano]
      else:
        genero[g].append(ano)
  else: continue

genero_periodo_Comedy = {}
genero_periodo_Sports = {}
genero_periodo_Drama = {}
genero_periodo_School = {}
genero_periodo_Shounen = {}
genero_periodo_Music = {}
genero_periodo_Romance = {}
genero_periodo_Sci_Fi = {}
genero_periodo_Adventure = {}
genero_periodo_Mystery = {}
genero_periodo_Fantasy = {}
genero_periodo_Action = {}
genero_periodo_Military = {}
genero_periodo_Magic = {}
genero_periodo_Supernatural = {}
genero_periodo_Vampire = {}
genero_periodo_Slice_of_Life = {}
genero_periodo_Demons = {}
genero_periodo_Historical = {}
genero_periodo_Super_Power = {}
genero_periodo_Mecha = {}
genero_periodo_Parody = {}
genero_periodo_Samurai = {}
genero_periodo_Seinen = {}
genero_periodo_Police = {}
genero_periodo_Psychological = {}
genero_periodo_Josei = {}
genero_periodo_Space = {}
genero_periodo_Kids = {}
genero_periodo_Shoujo_Ai = {}
genero_periodo_Ecchi = {}
genero_periodo_Shoujo = {}
genero_periodo_Horror = {}
genero_periodo_Shounen_Ai = {}
genero_periodo_Cars = {}
genero_periodo_Martial_Arts = {}
genero_periodo_Game = {}
genero_periodo_Thriller = {}
genero_periodo_Dementia = {}
genero_periodo_Harem = {}
genero_periodo_Hentai = {}
genero_periodo_Yaoi = {}
genero_periodo_Yuri = {}


for key in genero.keys():
  ano_min = int(min(genero[key]))
  ano_max = int(max(genero[key]))

  for i in range(ano_min, ano_max+1):
    i = str(i)
    if key == 'Comedy':
        genero_periodo_Comedy[i] = genero[key].count(i)
    elif key == 'Sports':
      genero_periodo_Sports[i] = genero[key].count(i)
    elif key == 'Drama':
      genero_periodo_Drama[i] = genero[key].count(i)
    elif key == 'School':
      genero_periodo_School[i] = genero[key].count(i)
    elif key == 'Shounen':
      genero_periodo_Shounen[i] = genero[key].count(i)
    elif key == 'Music':
      genero_periodo_Music[i] = genero[key].count(i)
    elif key == 'Romance':
      genero_periodo_Romance[i] = genero[key].count(i)
    elif key == 'Sci-Fi':
      genero_periodo_Sci_Fi[i] = genero[key].count(i)
    elif key == 'Adventure':
      genero_periodo_Adventure[i] = genero[key].count(i)
    elif key == 'Mystery':
      genero_periodo_Mystery[i] = genero[key].count(i)
    elif key == 'Fantasy':
      genero_periodo_Fantasy[i] = genero[key].count(i)
    elif key == 'Action':
      genero_periodo_Action[i] = genero[key].count(i)
    elif key == 'Military':
      genero_periodo_Military[i] = genero[key].count(i)
    elif key == 'Magic':
      genero_periodo_Magic[i] = genero[key].count(i)
    elif key == 'Supernatural':
      genero_periodo_Supernatural[i] = genero[key].count(i)
    elif key == 'Vampire':
      genero_periodo_Vampire[i] = genero[key].count(i)
    elif key == 'Slice of Life':
      genero_periodo_Slice_of_Life[i] = genero[key].count(i)
    elif key == 'Demons':
      genero_periodo_Demons[i] = genero[key].count(i)
    elif key == 'Historical':
      genero_periodo_Historical[i] = genero[key].count(i)
    elif key == 'Super Power':
      genero_periodo_Super_Power[i] = genero[key].count(i)
    elif key == 'Mecha':
      genero_periodo_Mecha[i] = genero[key].count(i)
    elif key == 'Parody':
      genero_periodo_Parody[i] = genero[key].count(i)
    elif key == 'Samurai':
      genero_periodo_Samurai[i] = genero[key].count(i)
    elif key == 'Seinen':
      genero_periodo_Seinen[i] = genero[key].count(i)
    elif key == 'Police':
      genero_periodo_Police[i] = genero[key].count(i)
    elif key == 'Psychological':
      genero_periodo_Psychological[i] = genero[key].count(i)
    elif key == 'Josei':
      genero_periodo_Josei[i] = genero[key].count(i)
    elif key == 'Space':
      genero_periodo_Space[i] = genero[key].count(i)
    elif key == 'Kids':
      genero_periodo_Kids[i] = genero[key].count(i)
    elif key == 'Shoujo Ai':
      genero_periodo_Shoujo_Ai[i] = genero[key].count(i)
    elif key == 'Ecchi':
      genero_periodo_Ecchi[i] = genero[key].count(i)
    elif key == 'Shoujo':
      genero_periodo_Shoujo[i] = genero[key].count(i)
    elif key == 'Horror':
      genero_periodo_Horror[i] = genero[key].count(i)
    elif key == 'Shounen Ai':
      genero_periodo_Shounen_Ai[i] = genero[key].count(i)
    elif key == 'Cars':
      genero_periodo_Cars[i] = genero[key].count(i)
    elif key == 'Martial Arts':
      genero_periodo_Martial_Arts[i] = genero[key].count(i)
    elif key == 'Game':
      genero_periodo_Game[i] = genero[key].count(i)
    elif key == 'Thriller':
      genero_periodo_Thriller[i] = genero[key].count(i)
    elif key == 'Dementia':
      genero_periodo_Dementia[i] = genero[key].count(i)
    elif key == 'Harem':
      genero_periodo_Harem[i] = genero[key].count(i)
    elif key == 'Hentai':
      genero_periodo_Hentai[i] = genero[key].count(i)
    elif key == 'Yaoi':
      genero_periodo_Yaoi[i] = genero[key].count(i)
    elif key == 'Yuri':
      genero_periodo_Yuri[i] = genero[key].count(i)


fig = plt.figure()

plt.plot_date(mdates.drange(datetime.datetime(int(next(iter(genero_periodo_Comedy))),1,1), datetime.datetime(2021,1,1), datetime.timedelta(days=365)), genero_periodo_Comedy.values(), 'g-')
plt.plot_date(mdates.drange(datetime.datetime(int(next(iter(genero_periodo_Sports))),1,1), datetime.datetime(2021,1,1), datetime.timedelta(days=365)), genero_periodo_Sports.values(), 'b-')
plt.plot_date(mdates.drange(datetime.datetime(int(next(iter(genero_periodo_Drama))),1,1), datetime.datetime(2021,1,1), datetime.timedelta(days=365)), genero_periodo_Drama.values(), 'b-')
plt.plot_date(mdates.drange(datetime.datetime(int(next(iter(genero_periodo_School))),1,1), datetime.datetime(2020,1,1), datetime.timedelta(days=365)), genero_periodo_School.values(), 'b-')
plt.plot_date(mdates.drange(datetime.datetime(int(next(iter(genero_periodo_Shounen))),1,1), datetime.datetime(2021,1,1), datetime.timedelta(days=365)), genero_periodo_Shounen.values(), 'b-')
plt.plot_date(mdates.drange(datetime.datetime(int(next(iter(genero_periodo_Music))),1,1), datetime.datetime(2021,1,1), datetime.timedelta(days=365)), genero_periodo_Music.values(), 'y-')
plt.plot_date(mdates.drange(datetime.datetime(int(next(iter(genero_periodo_Romance))),1,1), datetime.datetime(2020,1,1), datetime.timedelta(days=365)), genero_periodo_Romance.values(), 'b-')
plt.plot_date(mdates.drange(datetime.datetime(int(next(iter(genero_periodo_Sci_Fi))),1,1), datetime.datetime(2021,1,1), datetime.timedelta(days=365)), genero_periodo_Sci_Fi.values(), 'b-')
plt.plot_date(mdates.drange(datetime.datetime(int(next(iter(genero_periodo_Adventure))),1,1), datetime.datetime(2021,1,1), datetime.timedelta(days=365)), genero_periodo_Adventure.values(), 'b-')
plt.plot_date(mdates.drange(datetime.datetime(int(next(iter(genero_periodo_Mystery))),1,1), datetime.datetime(2020,1,1), datetime.timedelta(days=365)), genero_periodo_Mystery.values(), 'b-')
plt.plot_date(mdates.drange(datetime.datetime(int(next(iter(genero_periodo_Fantasy))),1,1), datetime.datetime(2021,1,1), datetime.timedelta(days=365)), genero_periodo_Fantasy.values(), 'c-')
plt.plot_date(mdates.drange(datetime.datetime(int(next(iter(genero_periodo_Action))),1,1), datetime.datetime(2021,1,1), datetime.timedelta(days=365)), genero_periodo_Action.values(), 'r-')
plt.plot_date(mdates.drange(datetime.datetime(int(next(iter(genero_periodo_Military))),1,1), datetime.datetime(2020,1,1), datetime.timedelta(days=365)), genero_periodo_Military.values(), 'b-')
plt.plot_date(mdates.drange(datetime.datetime(int(next(iter(genero_periodo_Magic))),1,1), datetime.datetime(2020,1,1), datetime.timedelta(days=365)), genero_periodo_Magic.values(), 'b-')
plt.plot_date(mdates.drange(datetime.datetime(int(next(iter(genero_periodo_Supernatural))),1,1), datetime.datetime(2020,1,1), datetime.timedelta(days=365)), genero_periodo_Supernatural.values(), 'b-')
plt.plot_date(mdates.drange(datetime.datetime(int(next(iter(genero_periodo_Vampire))),1,1), datetime.datetime(2020,1,1), datetime.timedelta(days=365)), genero_periodo_Vampire.values(), 'b-')
plt.plot_date(mdates.drange(datetime.datetime(int(next(iter(genero_periodo_Slice_of_Life))),1,1), datetime.datetime(2020,1,1), datetime.timedelta(days=365)), genero_periodo_Slice_of_Life.values(), 'm-')
plt.plot_date(mdates.drange(datetime.datetime(int(next(iter(genero_periodo_Demons))),1,1), datetime.datetime(2020,1,1), datetime.timedelta(days=365)), genero_periodo_Demons.values(), 'b-')
plt.plot_date(mdates.drange(datetime.datetime(int(next(iter(genero_periodo_Historical))),1,1), datetime.datetime(2021,1,1), datetime.timedelta(days=365)), genero_periodo_Historical.values(), 'b-')
plt.plot_date(mdates.drange(datetime.datetime(int(next(iter(genero_periodo_Super_Power))),1,1), datetime.datetime(2020,1,1), datetime.timedelta(days=365)), genero_periodo_Super_Power.values(), 'b-')
plt.plot_date(mdates.drange(datetime.datetime(int(next(iter(genero_periodo_Mecha))),1,1), datetime.datetime(2021,1,1), datetime.timedelta(days=365)), genero_periodo_Mecha.values(), 'b-')
plt.plot_date(mdates.drange(datetime.datetime(int(next(iter(genero_periodo_Parody))),1,1), datetime.datetime(2021,1,1), datetime.timedelta(days=365)), genero_periodo_Parody.values(), 'b-')
plt.plot_date(mdates.drange(datetime.datetime(int(next(iter(genero_periodo_Samurai))),1,1), datetime.datetime(2021,1,1), datetime.timedelta(days=365)), genero_periodo_Samurai.values(), 'b-')
plt.plot_date(mdates.drange(datetime.datetime(int(next(iter(genero_periodo_Seinen))),1,1), datetime.datetime(2020,1,1), datetime.timedelta(days=365)), genero_periodo_Seinen.values(), 'b-')
plt.plot_date(mdates.drange(datetime.datetime(int(next(iter(genero_periodo_Police))),1,1), datetime.datetime(2020,1,1), datetime.timedelta(days=365)), genero_periodo_Police.values(), 'b-')
plt.plot_date(mdates.drange(datetime.datetime(int(next(iter(genero_periodo_Psychological))),1,1), datetime.datetime(2020,1,1), datetime.timedelta(days=365)), genero_periodo_Psychological.values(), 'b-')
plt.plot_date(mdates.drange(datetime.datetime(int(next(iter(genero_periodo_Josei))),1,1), datetime.datetime(2020,1,1), datetime.timedelta(days=365)), genero_periodo_Josei.values(), 'b-')
plt.plot_date(mdates.drange(datetime.datetime(int(next(iter(genero_periodo_Space))),1,1), datetime.datetime(2020,1,1), datetime.timedelta(days=365)), genero_periodo_Space.values(), 'b-')
plt.plot_date(mdates.drange(datetime.datetime(int(next(iter(genero_periodo_Kids))),1,1), datetime.datetime(2020,1,1), datetime.timedelta(days=365)), genero_periodo_Kids.values(), 'b-')
plt.plot_date(mdates.drange(datetime.datetime(int(next(iter(genero_periodo_Shoujo_Ai))),1,1), datetime.datetime(2020,1,1), datetime.timedelta(days=365)), genero_periodo_Shoujo_Ai.values(), 'b-')
plt.plot_date(mdates.drange(datetime.datetime(int(next(iter(genero_periodo_Ecchi))),1,1), datetime.datetime(2020,1,1), datetime.timedelta(days=365)), genero_periodo_Ecchi.values(), 'b-')
plt.plot_date(mdates.drange(datetime.datetime(int(next(iter(genero_periodo_Shoujo))),1,1), datetime.datetime(2020,1,1), datetime.timedelta(days=365)), genero_periodo_Shoujo.values(), 'b-')
plt.plot_date(mdates.drange(datetime.datetime(int(next(iter(genero_periodo_Horror))),1,1), datetime.datetime(2020,1,1), datetime.timedelta(days=365)), genero_periodo_Horror.values(), 'b-')
plt.plot_date(mdates.drange(datetime.datetime(int(next(iter(genero_periodo_Shounen_Ai))),1,1), datetime.datetime(2020,1,1), datetime.timedelta(days=365)), genero_periodo_Shounen_Ai.values(), 'b-')
plt.plot_date(mdates.drange(datetime.datetime(int(next(iter(genero_periodo_Cars))),1,1), datetime.datetime(2019,1,1), datetime.timedelta(days=365)), genero_periodo_Cars.values(), 'b-')
plt.plot_date(mdates.drange(datetime.datetime(int(next(iter(genero_periodo_Martial_Arts))),1,1), datetime.datetime(2020,1,1), datetime.timedelta(days=365)), genero_periodo_Martial_Arts.values(), 'b-')
plt.plot_date(mdates.drange(datetime.datetime(int(next(iter(genero_periodo_Game))),1,1), datetime.datetime(2020,1,1), datetime.timedelta(days=365)), genero_periodo_Game.values(), 'b-')
plt.plot_date(mdates.drange(datetime.datetime(int(next(iter(genero_periodo_Thriller))),1,1), datetime.datetime(2020,1,1), datetime.timedelta(days=365)), genero_periodo_Thriller.values(), 'b-')
plt.plot_date(mdates.drange(datetime.datetime(int(next(iter(genero_periodo_Dementia))),1,1), datetime.datetime(2020,1,1), datetime.timedelta(days=365)), genero_periodo_Dementia.values(), 'b-')
plt.plot_date(mdates.drange(datetime.datetime(int(next(iter(genero_periodo_Harem))),1,1), datetime.datetime(2020,1,1), datetime.timedelta(days=365)), genero_periodo_Harem.values(), 'b-')
plt.plot_date(mdates.drange(datetime.datetime(int(next(iter(genero_periodo_Hentai))),1,1), datetime.datetime(2020,1,1), datetime.timedelta(days=365)), genero_periodo_Hentai.values(), 'b-')
plt.plot_date(mdates.drange(datetime.datetime(int(next(iter(genero_periodo_Yaoi))),1,1), datetime.datetime(2020,1,1), datetime.timedelta(days=365)), genero_periodo_Yaoi.values(), 'b-')
plt.plot_date(mdates.drange(datetime.datetime(int(next(iter(genero_periodo_Yuri))),1,1), datetime.datetime(2020,1,1), datetime.timedelta(days=365)), genero_periodo_Yuri.values(), 'b-')

plt.gca().xaxis.set_major_locator(mdates.YearLocator(5))
plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%Y'))

fig.autofmt_xdate(rotation=0)

plt.title('lançamento de gêneros de anime por ano.')

plt.xlabel('Ano de lançamento')
plt.ylabel('quantidade de lançamento')

red_patch = mpatches.Patch(color='red', label='Action')
green_patch = mpatches.Patch(color='green', label='Comedy')
yellow_patch = mpatches.Patch(color='yellow', label='Music')
pink_patch = mpatches.Patch(color='magenta', label='Slice of Life')
cyan_patch = mpatches.Patch(color='cyan', label='Fantasy')

plt.legend(handles=[green_patch, red_patch, yellow_patch, pink_patch, cyan_patch], loc= "upper center")

plt.subplots_adjust(left=0.0, right=3.0, bottom=0.0, top=1.0)
plt.show()

"""###Número de animes por ano."""

anos = {}
for i in range(1917, 2021):
  anos[str(i)] = 0
for i in base_animes.values:  
  if i[3].strip() != '0' : 
    ano = i[3].split(',')[-1].strip()

    if ano not in anos:
      anos[ano] = 1
    else:
      anos[ano] += 1
  else: continue

fig = plt.figure()
time = mdates.drange(datetime.datetime(1917,1,1), datetime.datetime(2021,1,1), datetime.timedelta(days=365))
plt.plot_date(time, anos.values(), 'b-')

plt.gca().xaxis.set_major_locator(mdates.YearLocator(5))
plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%Y'))

red_patch = mpatches.Patch(color='blue', label='Animes por ano')

plt.legend(handles=[red_patch], loc= "upper center")
plt.title('Quantidade de lançamentos de animes por ano.')
plt.subplots_adjust(left=0.0, right=3.0, bottom=0.0, top=1.0)

fig.show()

"""### Média de score
 (história, animação, som, personagens e engajamento).
"""

df5 = base_reviews
df5.boxplot(column=['story', 'animation', 'sound', 'character', 'enjoyment'])
plt.subplots_adjust(left=0.0, right=2.0, bottom=0.0, top=1.0)
plt.title('Média de scores.')
plt.show()

"""###Animes mais favoritados pelos usuários"""

fav_membros = {}
membros = []
anime_nomes = []

for i in base_animes.values:  
  fav_membros[i[7]] = i[1]

fav_membros = sorted(fav_membros.items(), reverse=True)

for i in range(0, 8666):
  fav_membros.pop()

for i in fav_membros:
  membros.append(i[0])
  anime_nomes.append(i[1])

plt.bar(anime_nomes,membros)
plt.xticks(rotation=90)
plt.xlabel('Nome do anime')
plt.ylabel('Quantidade de vezes que o anime foi salvo em favoritos (em milhões)')
plt.subplots_adjust(left=0.0, right=2.0, bottom=0.0, top=1.0)
plt.title('Animes mais favoritados.')
plt.show()

"""### Idade por quantidade de pessoas"""

df_reviewsprofile = base_reviews.join(other=base_profile.set_index('profile'), on= 'profile', lsuffix='_review', rsuffix='_profile')
def ano(data):
  if str(data) != 'nan' and ',' in data:
    current_year = date.today().year
    return current_year - int(data.split(',')[-1])
  else :return 0

df_reviewsprofile["idade"] = df_reviewsprofile["birthday"].apply(lambda x: ano(x))

plt.hist(x= df_reviewsprofile.idade.loc[df_reviewsprofile['idade'] != 0])
plt.title('Histograma da idade dos usuários que avaliaram algum anime.')
plt.xlabel('Idade')
plt.ylabel('Quantidade de pessoas.')
plt.subplots_adjust(left=0.0, right=1.5, bottom=0.0, top=1.0)
plt.show()

"""###Relação entre a idade/gênero do usuário e a avaliação dele sobre o anime"""

df = df_reviewsprofile.loc[df_reviewsprofile['idade'] != 0]
pivot = df.pivot_table(
    index=["gender"],
    columns=["idade"],
    values="score",
    aggfunc=np.average)

sns.heatmap(pivot,vmin=0, vmax=10, ax=plt.subplots_adjust(left=0.0, right=3.5, bottom=0.0, top=1.5))

def ano_decada(idade):
  if str(idade) != 'nan' and  idade != 0 and idade >= 10:
    return int(str(idade)[0]+'0')
  else:return 0

df_reviewsprofile["idade_decada"] = df_reviewsprofile["idade"].apply(lambda x: ano_decada(x))
df = df_reviewsprofile.loc[df_reviewsprofile['idade'] != 00]

pivot = df.pivot_table(
    index=["gender"],
    columns=["idade_decada"],
    values="score",
    aggfunc=np.average)

sns.heatmap(pivot,vmin=0, vmax=10, ax=plt.subplots_adjust(left=0.0, right=2.5, bottom=0.0, top=1.5),annot=True )

"""### Score médio por gênero de usuário"""

a = df_reviewsprofile.groupby('gender').mean()["score"]
plt.bar(['female', 'male','non-binary'], a.values)
plt.xlabel('Gênero')
plt.ylabel('Média de score')
plt.title('Grafico')
plt.subplots_adjust(left=0.0, right=1.5, bottom=0.0, top=1.0)
plt.show()

"""### Usuários agrupados por gênero"""

a = df_reviewsprofile.groupby('gender').count()['uid']
a

a = df_reviewsprofile.groupby('gender').count()['uid']
y =[(p*100)/sum(a.values) for p in a.values]

plt.bar(['female', 'male','non-binary'],y )
plt.xlabel('Gênero')
plt.ylabel('Porcentagem')
plt.title('Grafico da porcentagem de usuários por gênero.')
plt.subplots_adjust(left=0.0, right=1.5, bottom=0.0, top=1.0)
plt.show()

"""### Popularidade por scores"""

df_reviewsanimes = base_reviews.join(other=base_animes.set_index('uid'), on= 'anime_uid', lsuffix='_review', rsuffix='_anime')
df_reviewsanimes = df_reviewsanimes.iloc[:,[5,6,7,8,9,11,18]]
df_reviewsanimes = df_reviewsanimes.groupby('title').mean()
df_reviewsanimes = df_reviewsanimes.sort_values('popularity', ascending=True)
df_reviewsanimes.head(10)

df_reviewsanimes = df_reviewsanimes.sort_values('story', ascending=False)
df_reviewsanimes.head(10)

df_reviewsanimes = df_reviewsanimes.sort_values('animation', ascending=False)
df_reviewsanimes.head(10)

df_reviewsanimes = df_reviewsanimes.sort_values('sound', ascending=False)
df_reviewsanimes.head(10)

df_reviewsanimes = df_reviewsanimes.sort_values('character', ascending=False)
df_reviewsanimes.head(10)

df_reviewsanimes = df_reviewsanimes.sort_values('enjoyment', ascending=False)
df_reviewsanimes.head(10)

"""### Gráfico da preferência de gêneros de animes agrupados por gênero de usuário"""

base_animes

#algoritmo

genero_list = {}
for i in base_animes.values:
  generos = eval(i[5])
  for g in generos:
    if g not in genero_list:
      genero_list[g] = 0

male = dict(genero_list)
female = dict(genero_list)
nan = dict(genero_list)
non_binary = dict(genero_list)

for profile in base_profile.values:
  user_genre = profile[1]
  temp_list = dict(genero_list)
  if(type(profile[3]) == str):
    fav_list = ast.literal_eval(profile[3])
  else:
    fav_list = profile[3]
  for fav in fav_list:
    if(int(fav) in base_animes.uid.values):
      anime_genres =  base_animes.loc[base_animes['uid'] == int(fav)].iloc[0].genre
      if (type(anime_genres) == str):
        anime_genres = ast.literal_eval(anime_genres)
      
      for genre in anime_genres:
          temp_list[genre] = 1
  if (user_genre == "Male"):
    for add1 in male:
      male[add1] += temp_list[add1]
  elif (user_genre == "Female"):
    for add2 in female:
      female[add2] += temp_list[add2]
  elif (str(user_genre) == "nan"):
    for add3 in nan:
      nan[add3] += temp_list[add3]
  elif (str(user_genre) == "Non-Binary"):
    for add4 in nan:
      non_binary[add4] += temp_list[add4]

#gráfico
X = np.arange(len(genero_list))
ax = plt.subplot()
ax.bar(X- 0.2, list(male.values()),width=0.2, color='b', align='center',label = 'male')
ax.bar(X, list(female.values()),width=0.2, color='g', align='center', label = 'female')
ax.bar(X+ 0.2, list(nan.values()),width=0.2, color='k', align='center', label = 'nan')
ax.bar(X+ 0.4, list(non_binary.values()),width=0.2, color='r', align='center', label = 'non-binary')

ax.legend(loc='upper center')

plt.xticks(X, genero_list.keys(),rotation=90,fontsize=60)
plt.yticks(fontsize=60)

plt.title("Preferência dos gêneros", fontsize=70)
plt.xlabel("Gêneros",fontsize=70)
plt.ylabel("Quantidade de favoritos",fontsize=70)
plt.subplots_adjust(left=0.0, right=15.0, bottom=5.0, top=10.0)

plt.show()

"""### Grafico em nuvem das palavras mais utilizadas"""

#algoritmo

pln = spacy.load('en')
color_map = ListedColormap(['orange', 'green', 'red', 'magenta'])
cloud = WordCloud(background_color = 'white', max_words = 100, colormap=color_map)
text_reviews = pd.read_csv(r'/content/drive/MyDrive/PISI3/dataset/aprendizagem/text_reviews_06.csv')
lista = text_reviews.text

novalista = []
for i in lista:
  i = i.split()
  for j in i:
    novalista.append(j)

lista_token = []
for palavra in novalista:
  lista_token.append(str(palavra))

sem_stop = []
for palavra in lista_token:
  if pln.vocab[str(palavra)].is_stop == False:
    sem_stop.append(palavra)
  
sem_stop_limpo1 = []
sem_stop_limpo2 = []
sem_stop_limpo3 = []
for palavra in sem_stop:
  if len(sem_stop_limpo1) < 10000000:
    for i in palavra:
      if i.isnumeric() == False:
        sem_stop_limpo1.append(palavra)
        break
      else:
        break
  elif len(sem_stop_limpo2) < 10000000:
    for i in palavra:
      if i.isnumeric() == False:
        sem_stop_limpo2.append(palavra)
        break
      else:
        break
  elif len(sem_stop_limpo3) < 10000000:
    for i in palavra:
      if i.isnumeric() == False:
        sem_stop_limpo3.append(palavra)
        break
      else:
        break
  else:
    break

#Grafico
cloud = cloud.generate(' '.join(sem_stop_limpo1))
plt.figure(figsize= (15,15))
plt.imshow(cloud)
plt.axis('off')
plt.show()

"""# Aprendizado de máquina

## Pré-processamento dos reviews
"""

#importação de bibliotecas
!pip install num2words
!pip install unidecode

import pandas as pd
import numpy as np
import re
import nltk
import num2words
import string

from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import SnowballStemmer
from nltk.stem import WordNetLemmatizer
from num2words import num2words
from decimal import Decimal, DecimalException
from sklearn.feature_extraction.text import TfidfTransformer, TfidfVectorizer, CountVectorizer
from unidecode import unidecode

nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')

"""**O pré-processamento foi divido em 10 partes e cada uma delas foi executada e o resultado parcial foi armazenado, segue as etapas:**

* 00 - Processamento básico onde retiramos possíveis acentos, alguns simbolos e transformamos em texto completamente em letras minúsculas. 

* 01 - Nessa etapa é feito a remoção de possíveis espaços em branco ou quebras de linhas no texto.

* 02 - Ao análisar a base de dados foi observado que o atributo "text" contém alem do texto com as reviews os scores do anime avaliado, esse score ocupa as 14 primeiras palavras do text e nessa etapa foi feita a remoção dele.

* 03 - Nessa Etapa é feito a remoção das Stopwords dos textos.

* 04 - Na etapa 04 é feito a remoção de termos que "sobram" nos texto um exemplo pode ser "n't" e de letras únicas.

* 05 - Nessa Etapa é feito a remoção de símbolos.

* 06 - Nessa é feita a substituição de números por números por extenso.

* 07 - Nessa etapa é feito a aplicação da técnica de lematização.

* 08 - Na penúltima etapa é feito a aplicação da técnica de stemmer.

* 09 - Na ultima etapa foi feito a extração das 30 palavras mais importantes dos textos através do método tf-idf.

Carregamento da base
"""

text_reviews = pd.read_csv(r'/content/drive/MyDrive/PISI3/dataset/dataset-analise/reviews-limpo.csv')

"""00 - Processamento básico"""

text_reviews["text"] = text_reviews["text"].apply(lambda x: x.lower())
text_reviews["text"] = text_reviews["text"].apply(lambda x: unidecode(x))
text_reviews["text"] = text_reviews["text"].apply(lambda x: re.sub(',|\.|/|$|\(|\)|-|\+|:|•', '', x))

text_reviews[['anime_uid','text']].to_csv(r'/content/drive/MyDrive/PISI3/dataset/aprendizagem/text_reviews_00.csv',index=False)

"""01 - Remoção dos espaços em branco"""

text_reviews = pd.read_csv(r'/content/drive/MyDrive/PISI3/dataset/aprendizagem/text_reviews_00.csv')

text_reviews["text"] = text_reviews["text"].apply(lambda x: re.sub('\s+', ' ', x))

text_reviews.to_csv(r'/content/drive/MyDrive/PISI3/dataset/aprendizagem/text_reviews_01.csv',index=False)

"""02 - Removendo as 14 primeiras palavras pois são os scores."""

text_reviews = pd.read_csv(r'/content/drive/MyDrive/PISI3/dataset/aprendizagem/text_reviews_01.csv')

text_reviews["text"] = text_reviews["text"].apply(lambda x: ' '.join(word_tokenize(x)[14:]))

text_reviews.to_csv(r'/content/drive/MyDrive/PISI3/dataset/aprendizagem/text_reviews_02.csv',index=False)

"""03 - Remoção das stopwords"""

text_reviews = pd.read_csv(r'/content/drive/MyDrive/PISI3/dataset/aprendizagem/text_reviews_02.csv')

stop_words = stopwords.words('english') 
text_reviews["text"] = text_reviews["text"].apply(lambda x: ' '.join([word for word in word_tokenize(x) if word not in stop_words]))

text_reviews.to_csv(r'/content/drive/MyDrive/PISI3/dataset/aprendizagem/text_reviews_03.csv',index=False)

"""04 - Remoção de remanescentes"""

text_reviews = pd.read_csv(r'/content/drive/MyDrive/PISI3/dataset/aprendizagem/text_reviews_03.csv')

text_reviews["text"] = text_reviews["text"].apply(lambda x: ' '.join([word for word in word_tokenize(x) if not ( len(word)<2 and not word.isdigit() or "'" in word and len(word)<4)]))

text_reviews.to_csv(r'/content/drive/MyDrive/PISI3/dataset/aprendizagem/text_reviews_04.csv',index=False)

"""05 - Remoção de símbolos"""

text_reviews = pd.read_csv(r'/content/drive/MyDrive/PISI3/dataset/aprendizagem/text_reviews_04.csv')

def remove_simbols(text):
  symbols = string.punctuation
  for s in symbols:
    text = text.replace(s,'')
  return text

text_reviews["text"] = text_reviews["text"].apply(lambda x: remove_simbols(x))

text_reviews.to_csv(r'/content/drive/MyDrive/PISI3/dataset/aprendizagem/text_reviews_05.csv',index=False)

"""06 - Substituindo números por números por extenso"""

text_reviews = pd.read_csv(r'/content/drive/MyDrive/PISI3/dataset/aprendizagem/text_reviews_05.csv')

def replace_digits(text):
  text_temp = word_tokenize(text)
  for index, word in enumerate(text_temp):
    try:
      if word.isnumeric():
        text_temp[index] = num2words(word)
    except (ValueError, DecimalException):
      text_temp[index] = ''
      continue
  return ' '.join(text_temp)

text_reviews["text"] = text_reviews["text"].apply(lambda x: replace_digits(x))

text_reviews.to_csv(r'/content/drive/MyDrive/PISI3/dataset/aprendizagem/text_reviews_06.csv',index=False)

"""07 - Aplicação de lemmatizer"""

text_reviews = pd.read_csv(r'/content/drive/MyDrive/PISI3/dataset/aprendizagem/text_reviews_06.csv')

lemmatizer = WordNetLemmatizer()
text_reviews["text"] = text_reviews["text"].apply(lambda x: ' '.join([lemmatizer.lemmatize(word) for word in word_tokenize(x)]))

text_reviews.to_csv(r'/content/drive/MyDrive/PISI3/dataset/aprendizagem/text_reviews_07.csv',index=False)

"""08 - Aplicação de stemmer"""

text_reviews = pd.read_csv(r'/content/drive/MyDrive/PISI3/dataset/aprendizagem/text_reviews_07.csv')

stemmer = SnowballStemmer('english')
text_reviews["text"] = text_reviews["text"].apply(lambda x: ' '.join([stemmer.stem(word) for word in word_tokenize(x)]))

text_reviews.to_csv(r'/content/drive/MyDrive/PISI3/dataset/aprendizagem/text_reviews_08.csv',index=False)

text_reviews.text

"""09 - Aplicando o método TF-IDF"""

text_reviews = pd.read_csv(r'/content/drive/MyDrive/PISI3/dataset/aprendizagem/text_reviews_08.csv')

def TFIDF(text_reviews, n=30):
  tfIdfVectorizer=TfidfVectorizer(use_idf=True)
  tfIdf = tfIdfVectorizer.fit_transform([text_reviews])

  df = pd.DataFrame(tfIdf[0].T.todense(), index=tfIdfVectorizer.get_feature_names(), columns=["TF-IDF"])
  df = df.sort_values('TF-IDF', ascending=False)
  df = df.head(n)
  return ' '.join(df.index)

text_reviews["text"] = text_reviews["text"].apply(lambda x: TFIDF(x))

text_reviews.to_csv(r'/content/drive/MyDrive/PISI3/dataset/aprendizagem/text_reviews_processado.csv', index=False)

"""##K-means"""

!pip install plotly --upgrade
!pip install sklearn --upgrade
import sklearn
import plotly.express as px
import plotly.graph_objects as go
import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.decomposition import PCA
import joblib
import matplotlib.pyplot as plt

text_reviews = pd.read_csv(r'/content/drive/MyDrive/PISI3/dataset/aprendizagem/text_reviews_processado.csv').sample(10000)

"""Vetorizando"""

cv = CountVectorizer()
cv.fit(text_reviews['text'])
bag_of_words = cv.transform(text_reviews['text'])

"""redimensionando"""

pca = PCA(n_components=3)
pca.fit(bag_of_words.toarray())
bag_of_words_pca = pca.transform(bag_of_words.toarray())

"""escolhendo número de centroides


"""

wcss = []
for i in range(2,30):
  kmeans_text = KMeans(n_clusters=i, random_state=0)
  kmeans_text.fit(bag_of_words_pca)
  wcss.append(kmeans_text.inertia_)

grafico = px.line(x = range(2,30), y = wcss, labels={'x': 'Número de clusters', 'y': 'Distância'})
grafico.show()

"""Aplicando o algoritmo e recuperando os rotulos"""

kmeans_text = KMeans(n_clusters=10, random_state=0)
rotulos = kmeans_text.fit_predict(bag_of_words_pca)

"""Recuperando os centroides"""

centroides = kmeans_text.cluster_centers_

"""Vizualizando grupos"""

g = px.scatter_3d(bag_of_words_pca, bag_of_words_pca[:,0],bag_of_words_pca[:,1],bag_of_words_pca[:,2], color=rotulos, title='K-Means Clustering Results')
g.show()

#guarda os valores para serem usados nas clusterização do texto do usuário
joblib.dump(kmeans_text, r'/content/drive/MyDrive/PISI3/dataset/aprendizagem/kmeans.pkl') 

joblib.dump(cv, r'/content/drive/MyDrive/PISI3/dataset/aprendizagem/bag_of_words.pkl') 

joblib.dump(pca, r'/content/drive/MyDrive/PISI3/dataset/aprendizagem/pca.pkl') 

arq = open('/content/drive/MyDrive/PISI3/dataset/aprendizagem/rotulos.txt','w')
arq.write(f'{[r for r in rotulos]}')
arq.close()

arq = open('/content/drive/MyDrive/PISI3/dataset/aprendizagem/animes_uid.txt','w')
arq.write(f'{[r for r in text_reviews.anime_uid]}')
arq.close()

"""# Comunicação com a aplicação"""

#Acesso ao drive
from google.colab import drive
drive.mount('/content/drive')

!pip install flask-ngrok
!pip install num2words
!pip install unidecode
#flask import 
from flask_ngrok import run_with_ngrok
from flask import Flask
from flask import request

import pandas as pd
import numpy as np
import re
import nltk
import num2words
import string
import json



nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')

def processamento(text, n=30):
  #processamento básico
  text = text.lower()
  text = unidecode(text)
  text = re.sub(',|\.|/|$|\(|\)|-|\+|:|•', '', text)

  #Remove espaços
  text = re.sub('\s+', ' ', text)

  #Remove stopword
  stop_words = stopwords.words('english') 
  text = ' '.join([word for word in word_tokenize(text) if word not in stop_words])

  #remove remanescentes
  text = ' '.join([word for word in word_tokenize(text) if not ( len(word)<2 and not word.isdigit() or "'" in word and len(word)<4)])

  #remove simbolos
  symbols = string.punctuation
  for s in symbols:
    text = text.replace(s,'')

  #substituindo números por dígitos
  text_temp = word_tokenize(text)
  for index, word in enumerate(text_temp):
    try:
      if word.isnumeric():
        text_temp[index] = num2words(word)
    except (ValueError, DecimalException):
      text_temp[index] = ''
      continue
  text = ' '.join(text_temp)

  #Aplicando o lemmatizer
  lemmatizer = WordNetLemmatizer()
  text = ' '.join([lemmatizer.lemmatize(word) for word in word_tokenize(text)])

  #Aplicando o Stemmer
  stemmer = SnowballStemmer('english')
  text = ' '.join([stemmer.stem(word) for word in word_tokenize(text)])
  
  return text
  
def TFIDF(text_reviews, n=30):
  #Aplicando o TFI-DF
  tfIdfVectorizer=TfidfVectorizer(use_idf=True)
  tfIdf = tfIdfVectorizer.fit_transform([text_reviews])

  df = pd.DataFrame(tfIdf[0].T.todense(), index=tfIdfVectorizer.get_feature_names(), columns=["TF-IDF"])
  df = df.sort_values('TF-IDF', ascending=False)
  df = df.head(n)
  return ' '.join(df.index)
  

Flask.name= "AnimeCom"
app = Flask(__name__)
run_with_ngrok(app)   #starts ngrok when the app is run

@app.route("/clusterization")
def clusterization(): 

  #Recupera o texto do usuário
  my_json = json.loads(request.args.get('params'))
  text_user = my_json['text']
  print(text_user)

  #processamento 
  text_user = processamento(text_user)
  text_user = TFIDF(text_user)

  #vetoriza
  bag_of_words_model_loaded = joblib.load(r'/content/drive/MyDrive/PISI3/dataset/aprendizagem/bag_of_words.pkl')
  bag_of_words = bag_of_words_model_loaded.transform([text_user]*3)
  print(bag_of_words.shape[1])

  #redimensiona
  pca_loaded = joblib.load(r'/content/drive/MyDrive/PISI3/dataset/aprendizagem/pca.pkl')
  text_user = pca_loaded.transform(bag_of_words.toarray())

  #Carrega o clusterizador treinado
  model_loaded = joblib.load(r'/content/drive/MyDrive/PISI3/dataset/aprendizagem/kmeans.pkl')
  
  #Clusteriza o texto do usuário 
  rotulo_user = model_loaded.predict(text_user)
  print(f'rotulo = {rotulo_user}')

  #recupera os rotulos de treinamento e os uid dos animes em que a review usada para treino se refere
  rotulos_loaded = open(r'/content/drive/MyDrive/PISI3/dataset/aprendizagem/rotulos.txt')
  animes_loaded = open(r'/content/drive/MyDrive/PISI3/dataset/aprendizagem/animes_uid.txt')


  animes = [] #Uid dos animes que com base na review pertecem ao mesmo grupo que o texto do usuário
  animes_uid = eval(animes_loaded.readline())
  for i, rotulo in enumerate(eval(rotulos_loaded.readline())):
    if rotulo in rotulo_user:
      animes.append(animes_uid[i])

  print(animes)
  return json.dumps({'animes_uid': animes })

app.run()